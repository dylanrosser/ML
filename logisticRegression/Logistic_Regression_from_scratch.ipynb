{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch\n",
    "### Using the iris dataset\n",
    "\n",
    "### The sigmoid function:  $\\sigma(z)$\n",
    "$$\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "$$\\sigma'(z)= \\sigma(z)*(1- \\sigma(z))$$\n",
    "\n",
    "The logisitic regression model used here is:\n",
    "$$y=\\mathbf{\\sigma(X^\\top\\theta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data[:,:2]\n",
    "y=(iris.target !=0)*1 # multiplying True and False by 1 will give you integers!\n",
    "lr = 0.01 ## learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incase you want to try each function one at a time to see results\n",
    "def add_intercept(X):\n",
    "        intercept = np.ones(X.shape[0]).reshape(X.shape[0], 1) ## bias initial just a bunch of ones\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def loss(h, y):\n",
    "        return (- (y * np.log(h) + (1-y) * np.log(1-h)))\n",
    "\n",
    "def fit(X, y):\n",
    "    X=self.__add_intercept(X)\n",
    "    self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "    for i in range(self.iterations):\n",
    "        z=np.dot(X, self.theta).clip(-1e4, 1e4)\n",
    "        h=self.__sigmoid(z)\n",
    "\n",
    "        gradient = np.dot(X.T, (h-y)) / y.shape[0]\n",
    "        self.theta = self.theta - self.lr + gradient\n",
    "\n",
    "def predict_probs(theta):\n",
    "    h=sigmoid(np.dot(X, theta))\n",
    "    return h\n",
    "\n",
    "def predict(X, theta, threshold = 0.5):\n",
    "    h = predict_probs(X, theta)\n",
    "    return h >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, lr=0.01, iterations = 10000):\n",
    "        self.lr = lr\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    ## remember, python wont let you call methods that start with a __dunder externally  \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones(X.shape[0]).reshape(X.shape[0], 1) ## bias initial just a bunch of ones\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z): \n",
    "        return (1/(1+np.exp(-z)))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (- (y * np.log(h) + (1-y) * np.lop(1-h)))\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        X=self.__add_intercept(X)\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            z=np.dot(X, self.theta)\n",
    "            h=self.__sigmoid(z)\n",
    "\n",
    "            gradient = np.dot(X.T, (h-y)) / y.shape[0]\n",
    "            self.theta = self.theta - self.lr * gradient\n",
    "\n",
    "    def predict_probs(self, X, theta):\n",
    "        X=self.__add_intercept(X)\n",
    "        h=self.__sigmoid(np.dot(X, self.theta))\n",
    "        return h\n",
    "    \n",
    "    def predict(self, X, theta, threshold = 0.5):\n",
    "        h = self.predict_probs(X, theta)\n",
    "        return (h >= threshold)*1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.01, iterations=10000) # decrease learning rate to avoid vanishing gradient?\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X, model.theta)\n",
    "\n",
    "\n",
    "# calculate training accuracy\n",
    "acc = (predictions == y).mean()\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genertic ML algo:\n",
    "\n",
    "for i in number of interations: <br/>\n",
    "    -train <br/>\n",
    "    -loss <br/>\n",
    "    -gradient (RMSE) <br/>\n",
    "    -gradient descent (update weights) <br/>\n",
    "    -validation on the validation set <br/>\n",
    "    \n",
    "    \n",
    "after all of that you do a test of the test data to see how your model has done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.33%\n"
     ]
    }
   ],
   "source": [
    "# Simple logistic regression example using iris dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', max_iter=1000).fit(X, y)\n",
    "preds = clf.predict(X)\n",
    "\n",
    "# calculate training accuracy\n",
    "correct_pred = (preds == y).astype(float)\n",
    "acc = correct_pred.sum() / len(correct_pred)\n",
    "print('accuracy: {0:0.2f}%'.format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression on Iris Dataset\n",
    "**Using a Neural Net in Pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Logistic Regression on Iris Dataset using a Neural Net from Pytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "x_data = Variable(torch.Tensor(X))\n",
    "y_data = Variable(torch.Tensor(y))\n",
    "y_data = y_data.long()\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = LogisticRegression()\n",
    "# no need for explicit softmax layer, its included in CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 98.67%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# One more pass on training data\n",
    "y_pred = model(x_data)\n",
    "y_pred_softmax = F.log_softmax(y_pred, dim=1)\n",
    "_, y_pred_label = torch.max(y_pred_softmax, dim = 1)\n",
    "\n",
    "# calculate training accuracy\n",
    "correct_pred = (y_pred_label == y_data).float()\n",
    "acc = correct_pred.sum() / len(correct_pred)\n",
    "print('accuracy: {0:0.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on MNIST with PyTorch\n",
    "# https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 5378831.21it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 363941.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 3309307.63it/s]                            \n",
      "8192it [00:00, 116001.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "lr_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 1.8628113269805908. Accuracy: 70.\n",
      "Iteration: 1000. Loss: 1.5629479885101318. Accuracy: 77.\n",
      "Iteration: 1500. Loss: 1.3851027488708496. Accuracy: 79.\n",
      "Iteration: 2000. Loss: 1.194413185119629. Accuracy: 81.\n",
      "Iteration: 2500. Loss: 1.1063477993011475. Accuracy: 82.\n",
      "Iteration: 3000. Loss: 1.0484541654586792. Accuracy: 83.\n",
      "Iteration: 3500. Loss: 0.878743588924408. Accuracy: 83.\n",
      "Iteration: 4000. Loss: 0.9122090935707092. Accuracy: 84.\n",
      "Iteration: 4500. Loss: 0.8413637280464172. Accuracy: 84.\n",
      "Iteration: 5000. Loss: 0.867206871509552. Accuracy: 84.\n",
      "Iteration: 5500. Loss: 0.8396516442298889. Accuracy: 85.\n",
      "Iteration: 6000. Loss: 0.7621182799339294. Accuracy: 85.\n",
      "Iteration: 6500. Loss: 0.7023841142654419. Accuracy: 85.\n",
      "Iteration: 7000. Loss: 0.7873422503471375. Accuracy: 85.\n",
      "Iteration: 7500. Loss: 0.6074457764625549. Accuracy: 86.\n",
      "Iteration: 8000. Loss: 0.7212207913398743. Accuracy: 86.\n",
      "Iteration: 8500. Loss: 0.5700114369392395. Accuracy: 86.\n",
      "Iteration: 9000. Loss: 0.611498236656189. Accuracy: 86.\n",
      "Iteration: 9500. Loss: 0.5633618831634521. Accuracy: 86.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.666666666666668"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
